{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch # PyTorch package\n",
    "import torchvision.transforms as transforms # transform data\n",
    "from torchvision import transforms, datasets\n",
    "from pytorchtools import EarlyStopping\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import splitfolders\n",
    "\n",
    "#importing functions and parameters files\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.use(\"Agg\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions import model, lossFn, opt\n",
    "from params import bs, num_epochs, device, patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split the training folder into training and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run one time\n",
    "#splitfolders.ratio(\"images/training\", output=\"training-validation\",\n",
    "#    seed=1337, ratio=(.8, .2), group_prefix=None, move=False) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(1080),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flip', 'notflip']\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "train_data = datasets.ImageFolder('training-validation/train', transform = transform)\n",
    "val_data = datasets.ImageFolder('training-validation/val', transform = transform)\n",
    "test_data = datasets.ImageFolder('images/testing', transform = transform)\n",
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the train, validation, and test data loaders\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = bs)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size = bs)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainloader.dataset) // bs\n",
    "valSteps = len(valloader.dataset) // bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Number of color channels: 3\n",
      "Image size: 224 x 224\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(trainloader))\n",
    "print(f'Batch size: {inputs.shape[0]}')\n",
    "print(f'Number of color channels: {inputs.shape[1]}')\n",
    "print(f'Image size: {inputs.shape[2]} x {inputs.shape[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the images\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(inputs[i].permute(1, 2, 0))\n",
    "    plt.title([train_data.classes[classes[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    }
   ],
   "source": [
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using early stopping\n",
    "def train_model(model, bs, patience, num_epochs):\n",
    "\n",
    "\t# to store training history\n",
    "\ttrain_loss = []\n",
    "\ttrain_acc = []\n",
    "\tval_loss = []\n",
    "\tval_acc = []\n",
    "\n",
    "\t# initialize the early_stopping object\n",
    "\tearly_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "\t#training loop\n",
    "\t# loop over our epochs\n",
    "\tfor e in range(0, num_epochs):\n",
    "\t\t# set the model in training mode\n",
    "\t\tmodel.train()\n",
    "\t\t# initialize the total training and validation loss\n",
    "\t\ttotalTrainLoss = 0\n",
    "\t\ttotalValLoss = 0\n",
    "\t\t# initialize the number of correct predictions in the training\n",
    "\t\t# and validation step\n",
    "\t\ttrainCorrect = 0\n",
    "\t\tvalCorrect = 0\n",
    "\t\t# loop over the training set\n",
    "\t\tfor (x, y) in trainloader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# clear the gradients of all optimized variables\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\t# perform a forward pass and calculate the training loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\tloss = lossFn(pred, y.type(torch.float32).unsqueeze(1))\n",
    "\t\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t\t# and update the weights\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\t\t\t# add the loss to the total training loss so far and\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\ttotalTrainLoss += loss\n",
    "\t\t\ttrainCorrect += (torch.round(pred) == y).type(torch.float).mean().item()\n",
    "\n",
    "\t\t# Zeroing gradient, performing backpropagation, updating weights of the model\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tmodel.eval() # prep model for evaluation\n",
    "\t\t\t# loop over the validation set\n",
    "\t\t\tfor (x, y) in valloader:\n",
    "\t\t\t\t# send the input to the device\n",
    "\t\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\t\tpred = model(x)\n",
    "\t\t\t\ttotalValLoss += lossFn(pred, y.type(torch.float32).unsqueeze(1))\n",
    "\t\t\t\t# calculate the number of correct predictions\n",
    "\t\t\t\tvalCorrect += (torch.round(pred) == y).type(torch.float).mean().item()\n",
    "\t\n",
    "\t\t# calculate the average training and validation loss\n",
    "\t\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\t\tavgValLoss = totalValLoss / valSteps\n",
    "\t\t\n",
    "\t\t# calculate the training and validation accuracy\n",
    "\t\tavgtrainCorrect = trainCorrect / len(trainloader)\n",
    "\t\tavgvalCorrect = valCorrect / len(valloader)\n",
    "\t\t\n",
    "\t\t# update our training history\n",
    "\t\ttrain_loss.append(avgTrainLoss.cpu().detach().numpy())\n",
    "\t\ttrain_acc.append(avgtrainCorrect)\n",
    "\t\tval_loss.append(avgValLoss.cpu().detach().numpy())\n",
    "\t\tval_acc.append(avgvalCorrect)\n",
    "\t\t\n",
    "\t\t# print the model training and validation information\n",
    "\t\tprint(\"[INFO] Epoch: {}/{}\".format(e + 1, num_epochs))\n",
    "\t\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, avgtrainCorrect))\n",
    "\t\tprint(\"Valid loss: {:.6f}, Valid accuracy: {:.4f}\\n\".format(avgValLoss, avgvalCorrect))\n",
    "\n",
    "\t\t# early_stopping needs the validation loss to check if it has decresed, \n",
    "\t\t# and if it has, it will make a checkpoint of the current model\n",
    "\t\tearly_stopping(avgValLoss, model)\n",
    "        \n",
    "\t\tif early_stopping.early_stop:\n",
    "\t\t\tprint(\"Early stopping\")\n",
    "\t\t\tbreak\n",
    "\t\n",
    "\t# load the last checkpoint with the best model\n",
    "\tmodel.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\t\n",
    "\treturn  model, train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1/50\n",
      "Train loss: 0.676057, Train accuracy: 0.4969\n",
      "Valid loss: 0.569118, Valid accuracy: 0.7618\n",
      "\n",
      "Validation loss decreased (inf --> 0.569118).  Saving model ...\n",
      "[INFO] Epoch: 2/50\n",
      "Train loss: 0.333889, Train accuracy: 0.5299\n",
      "Valid loss: 0.266696, Valid accuracy: 0.8741\n",
      "\n",
      "Validation loss decreased (0.569118 --> 0.266696).  Saving model ...\n",
      "[INFO] Epoch: 3/50\n",
      "Train loss: 0.175747, Train accuracy: 0.5342\n",
      "Valid loss: 0.418893, Valid accuracy: 0.8017\n",
      "\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[INFO] Epoch: 4/50\n",
      "Train loss: 0.125097, Train accuracy: 0.5202\n",
      "Valid loss: 0.174736, Valid accuracy: 0.9225\n",
      "\n",
      "Validation loss decreased (0.266696 --> 0.174736).  Saving model ...\n",
      "[INFO] Epoch: 5/50\n",
      "Train loss: 0.110850, Train accuracy: 0.5251\n",
      "Valid loss: 0.142808, Valid accuracy: 0.9328\n",
      "\n",
      "Validation loss decreased (0.174736 --> 0.142808).  Saving model ...\n",
      "[INFO] Epoch: 6/50\n",
      "Train loss: 0.075110, Train accuracy: 0.5326\n",
      "Valid loss: 0.084976, Valid accuracy: 0.9538\n",
      "\n",
      "Validation loss decreased (0.142808 --> 0.084976).  Saving model ...\n",
      "[INFO] Epoch: 7/50\n",
      "Train loss: 0.043946, Train accuracy: 0.5281\n",
      "Valid loss: 0.169237, Valid accuracy: 0.9309\n",
      "\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[INFO] Epoch: 8/50\n",
      "Train loss: 0.046438, Train accuracy: 0.5288\n",
      "Valid loss: 0.056957, Valid accuracy: 0.9685\n",
      "\n",
      "Validation loss decreased (0.084976 --> 0.056957).  Saving model ...\n",
      "[INFO] Epoch: 9/50\n",
      "Train loss: 0.069709, Train accuracy: 0.5276\n",
      "Valid loss: 0.072589, Valid accuracy: 0.9621\n",
      "\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[INFO] Epoch: 10/50\n",
      "Train loss: 0.037667, Train accuracy: 0.5311\n",
      "Valid loss: 0.075438, Valid accuracy: 0.9686\n",
      "\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[INFO] Epoch: 11/50\n",
      "Train loss: 0.168955, Train accuracy: 0.5231\n",
      "Valid loss: 0.184251, Valid accuracy: 0.9397\n",
      "\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[INFO] Epoch: 12/50\n",
      "Train loss: 0.066559, Train accuracy: 0.5287\n",
      "Valid loss: 0.102427, Valid accuracy: 0.9559\n",
      "\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[INFO] Epoch: 13/50\n",
      "Train loss: 0.017221, Train accuracy: 0.5313\n",
      "Valid loss: 0.068199, Valid accuracy: 0.9664\n",
      "\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[INFO] Epoch: 14/50\n",
      "Train loss: 0.027094, Train accuracy: 0.5317\n",
      "Valid loss: 0.099439, Valid accuracy: 0.9474\n",
      "\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[INFO] Epoch: 15/50\n",
      "Train loss: 0.017469, Train accuracy: 0.5302\n",
      "Valid loss: 0.066261, Valid accuracy: 0.9685\n",
      "\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[INFO] Epoch: 16/50\n",
      "Train loss: 0.022207, Train accuracy: 0.5325\n",
      "Valid loss: 0.075884, Valid accuracy: 0.9706\n",
      "\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[INFO] Epoch: 17/50\n",
      "Train loss: 0.016981, Train accuracy: 0.5326\n",
      "Valid loss: 0.166901, Valid accuracy: 0.9522\n",
      "\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[INFO] Epoch: 18/50\n",
      "Train loss: 0.012880, Train accuracy: 0.5377\n",
      "Valid loss: 0.040569, Valid accuracy: 0.9729\n",
      "\n",
      "Validation loss decreased (0.056957 --> 0.040569).  Saving model ...\n",
      "[INFO] Epoch: 19/50\n",
      "Train loss: 0.003691, Train accuracy: 0.5305\n",
      "Valid loss: 0.057164, Valid accuracy: 0.9664\n",
      "\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[INFO] Epoch: 20/50\n",
      "Train loss: 0.003809, Train accuracy: 0.5286\n",
      "Valid loss: 0.089671, Valid accuracy: 0.9642\n",
      "\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[INFO] Epoch: 21/50\n",
      "Train loss: 0.020498, Train accuracy: 0.5273\n",
      "Valid loss: 0.120731, Valid accuracy: 0.9477\n",
      "\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[INFO] Epoch: 22/50\n",
      "Train loss: 0.016329, Train accuracy: 0.5314\n",
      "Valid loss: 0.093424, Valid accuracy: 0.9621\n",
      "\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[INFO] Epoch: 23/50\n",
      "Train loss: 0.008087, Train accuracy: 0.5305\n",
      "Valid loss: 0.051831, Valid accuracy: 0.9689\n",
      "\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[INFO] Epoch: 24/50\n",
      "Train loss: 0.032150, Train accuracy: 0.5362\n",
      "Valid loss: 0.046032, Valid accuracy: 0.9724\n",
      "\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[INFO] Epoch: 25/50\n",
      "Train loss: 0.010732, Train accuracy: 0.5277\n",
      "Valid loss: 0.069092, Valid accuracy: 0.9690\n",
      "\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[INFO] Epoch: 26/50\n",
      "Train loss: 0.000677, Train accuracy: 0.5343\n",
      "Valid loss: 0.069982, Valid accuracy: 0.9727\n",
      "\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[INFO] Epoch: 27/50\n",
      "Train loss: 0.026204, Train accuracy: 0.5320\n",
      "Valid loss: 0.079715, Valid accuracy: 0.9602\n",
      "\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[INFO] Epoch: 28/50\n",
      "Train loss: 0.060241, Train accuracy: 0.5275\n",
      "Valid loss: 0.046333, Valid accuracy: 0.9685\n",
      "\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model, AvgTrainLoss, avgtrainCorrect, avgValLoss, avgvalCorrect = train_model(model, bs, patience, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(AvgTrainLoss)+1),AvgTrainLoss, label='Training Loss')\n",
    "plt.plot(range(1,len(avgValLoss)+1),avgValLoss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = avgValLoss.index(min(avgValLoss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(AvgTrainLoss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total time taken to train the model: 5644.31s\n"
     ]
    }
   ],
   "source": [
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x676 and 10816x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m     test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testloader)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest set: Avg. loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     14\u001b[0m             test_loss, correct, \u001b[38;5;28mlen\u001b[39m(testloader),\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(testloader)))\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossFn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, lossFn, testloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m testloader:\n\u001b[1;32m----> 8\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      9\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lossFn(out, y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(out)\u001b[38;5;241m.\u001b[39meq(y)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\OneDrive\\Documents\\QmZ3XBnlsDdbTh9d\\functions.py:51\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 51\u001b[0m   main_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_activation(main_output)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\dansh\\OneDrive\\Documents\\QmZ3XBnlsDdbTh9d\\functions.py:44\u001b[0m, in \u001b[0;36mNet.main_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x676 and 10816x1024)"
     ]
    }
   ],
   "source": [
    "def test_model(model, lossFn, testloader):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (x, y) in testloader:\n",
    "      out = model(x).flatten()\n",
    "      test_loss += lossFn(out, y.type(torch.float32).unsqueeze(1))\n",
    "      correct += torch.round(out).eq(y).sum()\n",
    "      \n",
    "    test_loss /= len(testloader)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testloader),\n",
    "            100. * correct / len(testloader)))\n",
    "    \n",
    "test_model(model, lossFn, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "print(classification_report(test_data.targets.numpy(),np.array(out), target_names=test_data.classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
