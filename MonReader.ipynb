{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # PyTorch package\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import *\n",
    "from torchsummary import summary\n",
    "from pytorchtools import EarlyStopping\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import splitfolders\n",
    "import os\n",
    "import re\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.use(\"Agg\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#importing functions and parameters files\n",
    "from functions import Net, ImageSequenceDataset, ImageSequenceClassifier\n",
    "from params import bs, num_epochs, device, patience, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split the training folder into training and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run one time\n",
    "#splitfolders.ratio(\"images/training\", output=\"training-validation\",\n",
    "#    seed=1337, ratio=(.8, .2), group_prefix=None, move=False) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(1080),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flip', 'notflip']\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "train_data = datasets.ImageFolder('training-validation/train', transform = transform)\n",
    "val_data = datasets.ImageFolder('training-validation/val', transform = transform)\n",
    "test_data = datasets.ImageFolder('images/testing', transform = transform)\n",
    "seq_data = datasets.ImageFolder('images/training', transform = transform)\n",
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the train, validation, and test data loaders\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = bs)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size = bs)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainloader.dataset) // bs\n",
    "valSteps = len(valloader.dataset) // bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input size: torch.Size([16, 3, 224, 224]), train class size: torch.Size([16])\n",
      "val input size: torch.Size([16, 3, 224, 224]), val class size: torch.Size([16])\n",
      "test input size: torch.Size([16, 3, 224, 224]), test class size: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#inputs, classes = next(iter(trainloader))\n",
    "#print(f'Batch size: {inputs.shape[0]}')\n",
    "#print(f'Number of color channels: {inputs.shape[1]}')\n",
    "#print(f'Image size: {inputs.shape[2]} x {inputs.shape[3]}')\n",
    "\n",
    "train_inputs, train_classes = next(iter(trainloader))\n",
    "print(f'train input size: {train_inputs.shape}, train class size: {train_classes.shape}')\n",
    "val_inputs, val_classes = next(iter(valloader))\n",
    "print(f'val input size: {val_inputs.shape}, val class size: {val_classes.shape}')\n",
    "test_inputs, test_classes = next(iter(testloader))\n",
    "print(f'test input size: {test_inputs.shape}, test class size: {test_classes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the images\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(train_inputs[i].permute(1, 2, 0))\n",
    "    plt.title([train_data.classes[train_classes[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    }
   ],
   "source": [
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling our model\n",
    "model = Net()\n",
    "\n",
    "# initialize our optimizer and loss function\n",
    "# specify loss function\n",
    "lossFn = nn.BCELoss() \n",
    "# specify optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using early stopping\n",
    "def train_model(model, bs, patience, num_epochs):\n",
    "\n",
    "\t# to store training history\n",
    "\ttrain_loss = []\n",
    "\ttrain_acc = []\n",
    "\tval_loss = []\n",
    "\tval_acc = []\n",
    "\n",
    "\t# initialize the early_stopping object\n",
    "\tearly_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "\t#training loop\n",
    "\t# loop over our epochs\n",
    "\tfor e in range(0, num_epochs):\n",
    "\t\t# set the model in training mode\n",
    "\t\tmodel.train()\n",
    "\t\t# initialize the total training and validation loss\n",
    "\t\ttotalTrainLoss = 0\n",
    "\t\ttotalValLoss = 0\n",
    "\t\t# initialize the number of correct predictions in the training\n",
    "\t\t# and validation step\n",
    "\t\ttrainCorrect = 0\n",
    "\t\tvalCorrect = 0\n",
    "\t\t# loop over the training set\n",
    "\t\tfor (x, y) in trainloader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# clear the gradients of all optimized variables\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\t# perform a forward pass and calculate the training loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\tloss = lossFn(pred, y.type(torch.float32).unsqueeze(1))\n",
    "\t\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t\t# and update the weights\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\t\t\t# add the loss to the total training loss so far and\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\ttotalTrainLoss += loss\n",
    "\t\t\ttrainCorrect += (torch.round(pred) == y).type(torch.float).mean().item()\n",
    "\n",
    "\t\t# Zeroing gradient, performing backpropagation, updating weights of the model\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tmodel.eval() # prep model for evaluation\n",
    "\t\t\t# loop over the validation set\n",
    "\t\t\tfor (x, y) in valloader:\n",
    "\t\t\t\t# send the input to the device\n",
    "\t\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\t\tpred = model(x)\n",
    "\t\t\t\ttotalValLoss += lossFn(pred, y.type(torch.float32).unsqueeze(1))\n",
    "\t\t\t\t# calculate the number of correct predictions\n",
    "\t\t\t\tvalCorrect += (torch.round(pred) == y).type(torch.float).mean().item()\n",
    "\t\n",
    "\t\t# calculate the average training and validation loss\n",
    "\t\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\t\tavgValLoss = totalValLoss / valSteps\n",
    "\t\t\n",
    "\t\t# calculate the training and validation accuracy\n",
    "\t\tavgtrainCorrect = trainCorrect / len(trainloader)\n",
    "\t\tavgvalCorrect = valCorrect / len(valloader)\n",
    "\t\t\n",
    "\t\t# update our training history\n",
    "\t\ttrain_loss.append(avgTrainLoss.cpu().detach().numpy())\n",
    "\t\ttrain_acc.append(avgtrainCorrect)\n",
    "\t\tval_loss.append(avgValLoss.cpu().detach().numpy())\n",
    "\t\tval_acc.append(avgvalCorrect)\n",
    "\t\t\n",
    "\t\t# print the model training and validation information\n",
    "\t\tprint(\"[INFO] Epoch: {}/{}\".format(e + 1, num_epochs))\n",
    "\t\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, avgtrainCorrect))\n",
    "\t\tprint(\"Valid loss: {:.6f}, Valid accuracy: {:.4f}\\n\".format(avgValLoss, avgvalCorrect))\n",
    "\n",
    "\t\t# early_stopping needs the validation loss to check if it has decresed, \n",
    "\t\t# and if it has, it will make a checkpoint of the current model\n",
    "\t\tearly_stopping(avgValLoss, model)\n",
    "        \n",
    "\t\tif early_stopping.early_stop:\n",
    "\t\t\tprint(\"Early stopping\")\n",
    "\t\t\tbreak\n",
    "\t\n",
    "\t# load the last checkpoint with the best model\n",
    "\tmodel.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\t\n",
    "\treturn  model, train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, AvgTrainLoss, avgtrainCorrect, avgValLoss, avgvalCorrect = \n",
    "#train_model(model, bs, patience, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "#fig = plt.figure(figsize=(10,8))\n",
    "#plt.plot(range(1,len(AvgTrainLoss)+1),AvgTrainLoss, label='Training Loss')\n",
    "#plt.plot(range(1,len(avgValLoss)+1),avgValLoss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "#minposs = avgValLoss.index(min(avgValLoss))+1 \n",
    "#plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.ylim(0, 0.5) # consistent scale\n",
    "#plt.xlim(0, len(AvgTrainLoss)+1) # consistent scale\n",
    "#plt.grid(True)\n",
    "#plt.legend()\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "#fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total time taken to train the model: 37.75s\n"
     ]
    }
   ],
   "source": [
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, lossFn, testloader):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  pred_list, true_list = [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (x, y) in testloader:\n",
    "      if len(x.shape) == 3:\n",
    "        x = torch.unsqueeze(x,0)\n",
    "      out = model(x).flatten()\n",
    "      if type(x)!=type(y):\n",
    "        y=torch.Tensor([y])\n",
    "      test_loss += lossFn(out, y.type(torch.float32))\n",
    "      correct += torch.round(out).eq(y).sum()\n",
    "      pred_list.append(torch.round(out))\n",
    "      true_list.append(y.type(torch.float32))\n",
    "\n",
    "      # Print every 100 iterations\n",
    "      if (i + 1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1}/{len(testloader)}\")\n",
    "      \n",
    "    test_loss /= len(testloader)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testloader),\n",
    "            100. * correct / len(testloader)))\n",
    "    \n",
    "  return pred_list, true_list\n",
    "\n",
    "#predictions, labels = test_model(model, lossFn, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a classification report\n",
    "#print(classification_report([i.item() for i in predictions], [i.item() for i in labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying Sequence of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To write a list of file names\n",
    "\n",
    "#Define the path to the folder\n",
    "#folder_path = \"images/training\"\n",
    "\n",
    "#Function to get file names with a specific extension from a folder\n",
    "#def get_file_names(folder_path):\n",
    "#    file_names = []\n",
    "#    for item in os.listdir(folder_path):\n",
    "#        item_path = os.path.join(folder_path, item)\n",
    "#        if os.path.isfile(item_path):\n",
    "#            file_names.append(item)\n",
    "#    return file_names\n",
    "\n",
    "# Get file names from the folder\n",
    "#file_names = get_file_names(folder_path)\n",
    "\n",
    "#print(\"File names:\", file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable sequence size\n",
    "\n",
    "# Extract sequence size from file names first 4 characters\n",
    "#sequence_size = len(re.findall(r'^\\d{4}', file_names[0]))\n",
    "\n",
    "#print(\"Sequence size:\", sequence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mlossFn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "sequence_length = 5\n",
    "\n",
    "# Create a DataLoader\n",
    "seqloader = DataLoader(seq_data, batch_size= bs*sequence_length , shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = 2 \n",
    "hidden_size = 128 # Adjust \n",
    "model = ImageSequenceClassifier(num_classes, hidden_size)\n",
    "\n",
    "# Iterate through the dataloader and pass sequences to the model\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in seqloader:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(images.unsqueeze(1))\n",
    "        loss = lossFn(outputs, labels.unsqueeze(1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
